<h2 align="center">
<p>Vision Transfomer ü§ñ</p>
</h2>

## üÜï Actualizaci√≥n
- 16/03/21: Soporte para Tensorflow 2.3 y 2.4 (Multi-head attention no esta disponible en TF 2.3 de manera nativa). Implementaci√≥n basada de [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/).

 TODO:
- ‚úÖ Implementaci√≥n en Tensorflow
- ‚¨úÔ∏è Integraci√≥n con Captum
- ‚¨úÔ∏è Implementaci√≥n en Pytorch
- ‚¨úÔ∏è Serializaci√≥n del modelo (despliegue)

## ‚ÑπÔ∏è Introducci√≥n
Si bien la arquitectura Transformer se ha convertido en el est√°ndar de facto para tareas de procesamiento del lenguaje, sus aplicaciones en visi√≥n por computadora siguen siendo limitadas.  En visi√≥n, los m√©todos de atenci√≥n se aplican en conjunto con redes convolucionales, o son utilizados para reemplazar ciertas componentes de las redes convolucionales manteniendo la estructura general en su lugar.  

En este repositorio se presenta una implementaci√≥n de la arquitectura [Vision Transfomer](https://arxiv.org/abs/2010.11929) que demuestra independencia de las CNN en tareas de visi√≥n, y usando solamente Transformers,  aplicados directamente a secuencias de parches de una imagen, puede realizar muy bien varias tareas de clasificaci√≥n de im√°genes. Vision Transformer (ViT) logra excelentes resultados en comparaci√≥n con las redes convolucionales de √∫ltima generaci√≥n, al tiempo que requieren sustancialmente menos recursos computacionales para entrenarse.

La investigaci√≥n de Vision Transformer se suma a la de muchos otros pioneros en el campo de NLP&CV, por mencionar algunos como DERT o DeiT. Si desea analizar el paper original en espa√±ol, consulte [este](src/ViT_resumen.pdf) documento que logre traducir y extender con algunas t√©cnicas de explicabilidad que pr√≥ximamente contar√°n con su respectiva implementaci√≥n (posiblemente usando [Captum](https://captum.ai/)).


## üß† Modelo

La implementaci√≥n del modelo que se presenta en este repositorio ha sido levemente modificada para poder ser entrenada m√°s rapidamente (debido a que se requiere un hardware relativamente potente). Se sigui√≥ en la medida de lo posible los lineamientos del paper original. 

A continuaci√≥n se muestra el funcionamiento de la arquitectura:

<p align="center">
  <img src="./src/vit.gif" />
</p>

La arquitectura contiene dos piezas clave:
- **Embeddings de entrada**: Es una concatenaci√≥n de dos embeddings, un embedding posicional que brindar√° la posici√≥n relativa de un parche respecto a la imagen completa, y un embedding proyecci√≥n que reduce la longitud de entrada hacia el bloque Transfomer para poder acelerar el c√≥mputo. Una longitud de entrada mayor, requiere m√°s c√≥mputo.
- **MLP head**: Es una red densa normal, que se encargar√° de procesar la salida del Transformer y poder realizar la clasificaci√≥n de manera m√°s sencilla (haciendo uso de la representaci√≥n espacial obtenida por el Transformer).

Si desea analizar la arquitectura m√°s a fondo, si√©ntase libre de editar el [modelo](model.py).

## üìÅ Dataset

El dataset usado fue CIFAR10, consta de 60000 im√°genes en color, resoluci√≥n 32x32 y agrupada en 10 clases, con 6000 im√°genes por clase.

A continuaci√≥n se muestran las categor√≠as disponibles:

<p align="center">
  <img src="./src/cifar10.png" />
</p>

Si√©ntase libre de usar *CIFAR100* u otro tipo de dataset.

## ‚ñ∂ Demo
Instale las dependencias del archivo `requirements.txt` con el siguiente comando:
```bash
pip install -r requirements.txt --no-cache-dir
```

Para verificar la integridad de las componentes del modelo, ejecute el siguiente comando:
```bash
python model.py
```
Inmediatamente, usted observar√° las siguientes im√°genes:
<p align="center">Original</p>
<p align="center">
<img src="./src/image.png" />
</p>

<p align="center">Parches de imagen (input del modelo)</p>
<p align="center">
<img src="./src/patches.png" />
</p>



Para poder entrenar el modelo, ejecute el siguiente comando:

```bash
python train.py
```

Despu√©s de 10 √©pocas (1 hora aproximadamente con la GPU a tope), se consigui√≥ un accuracy de aproximadamente 62.62% y un top 5 accuracy de aproximadamente 96.47% en el dataset de CIFAR10. Si desea un mayor performance, elija un dataset mucho m√°s grande (Imagenet o CIFAR100) y aumente considerablemente el n√∫mero de √©pocas.

Para poder hacer una predicci√≥n con el modelo entrenado, ejecute el siguiente comando:

```bash
python test.py
```
Se mostrar√° la siguiente ventana:
<p align="center">
  <img src="./src/result.png" />
</p>
