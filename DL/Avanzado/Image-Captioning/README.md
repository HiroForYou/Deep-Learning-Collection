<h2 align="center">
<p>Image Caption Generation üñºÔ∏è</p>
</h2>

 TODO:
- ‚úÖ Implementaci√≥n en Pytorch
- ‚¨úÔ∏è Integraci√≥n con Captum para interpretabilidad del modelo
- ‚¨úÔ∏è A√±adir mecanismos de atenci√≥n

## ‚ÑπÔ∏è Introducci√≥n
La *generaci√≥n de subt√≠tulos en im√°genes* es un problema muy conocido de Deep Learning en la que muchos investigadores de las √°reas de NLP y CV se han visto involucrados desde mucho antes que surgiesen arquitecturas novedosas como los Transfomers o las GAN's. 

En este repositorio se presenta una implementaci√≥n basada CNN's y LSTM's. Mejoras adicionales ser√≠an la inclusi√≥n de mecanismos de atenci√≥n como en el [paper](https://arxiv.org/abs/1502.03044) de Bengio et al. publicado en 2015 que ha sido el punto partida para la inclusi√≥n de Transformers en este tipo de problema.

## üß† Modelo

La implementaci√≥n del modelo que se presenta en este repositorio ha sido modificada a partir de [este](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning) otro repositorio. Sin embargo, todav√≠a se requiere un hardware relativamente potente y bastante paciencia para el entrenamiento. La arquitectura consta de un *Encoder CNN* construido apartir del modelo preentrenado *Inception v3*, y de un *Decoder LSTM* que ser√° la encargada de interpretar las car√°cteristicas visuales obtenidas por el Encoder y as√≠ generar una descripi√≥n de la imagen en cuesti√≥n.  

A continuaci√≥n se muestran m√°s detalles de la arquitectura:

<p align="center">
  <img src="./src/model.png" />
</p>

Si desea analizar la arquitectura m√°s a fondo, si√©ntase libre de editar el [modelo](model.py).

## üìÅ Dataset

Se uso un dataset de Kaggle llamado ["Flickr8k-Images-Captions"](https://www.kaggle.com/dataset/e1cd22253a9b23b073794872bf565648ddbe4f17e7fa9e74766ad3707141adeb), el cual contiene diversas im√°genes acompa√±adas de una breve descripci√≥n contextual escrita. El archivo pesa aproximadamente 1GB. Descompr√≠malo en la carpeta `data`.

Si√©ntase libre de usar *COCO* u otro tipo de dataset que contexta una descripci√≥n contextual.

## ‚ñ∂ Demo
Instale las dependencias del archivo `requirements.txt` con el siguiente comando:
```bash
pip install -r requirements.txt --no-cache-dir
```

Para poder entrenar el modelo, ejecute el siguiente comando:

```bash
python train.py
```
Para visualizar c√≥mo disminuye la p√©rdida, ejecute **Tensorboard**:
```bash
tensorboard --logdir "./"
```

Finalizada cada √©poca, se mostrar√° un resultado gr√°fico. A continuaci√≥n se muestra la prueba en dos im√°genes despu√©s de 8 √©pocas (aproximadamente 2.5 horas de entrenamiento en una 1050Ti).
<p align="center">Resultado 1</p>
<p align="center">
<img src="./test_images/michi_resultado.png" />
</p>

<p align="center">Resultado 2</p>
<p align="center">
<img src="./test_images/wiki_resultado.png" />
</p>

Como se puede observar, si bien no acierta en el primer ejemplo, en el segundo ejemplo si se logra dar una aproximaci√≥n regularmente buena (*un hombre con una camisa azul est√° de pie sobre una roca con vistas a un r√≠o*). Si desea obtener mejores resultados, entrene por m√°s √©pocas y tenga mucha paciencia :)! 